#!/bin/bash

JOBID="$1"
USER="$2"

source /var/spool/torque/mom_scripts/torque-sw-paths.sh
source /var/spool/torque/mom_scripts/torque-scratch-mgr.sh

# custom epilogue scripts
SCRIPTS_DIR='/var/spool/torque/mom_priv/epilogue.d'
if [ -d "${SCRIPTS_DIR}" ]; then
	for F in "${SCRIPTS_DIR}"/*.sh; do
		if [ -x "${F}" ]; then
			source "${F}" || exit
		fi
	done
fi

if [ -z "$(echo $0 | grep .parallel)" ]; then
# For shared scratch, master node is the only one that handles it
	SCRATCHES="/scratch /scratch.ssd /scratch.shared" # head node
else
# Other types of scratch are all local, thus handled everywhere
	SCRATCHES="/scratch /scratch.ssd" # slave nodes
fi

for scr in $SCRATCHES; do
	if [ -d "$scr/$USER/job_$JOBID" ]; then
		$RMDIR --ignore-fail-on-non-empty "$scr/$USER/job_$JOBID" 2>/dev/null

		if [ -d "$scr/$USER/job_$JOBID" ]; then

			left_over=$(dead_quota_size $scr/$USER/job_$JOBID);

			if [ ! -z $left_over ]; then
				if [ $left_over -gt 0 ]; then
					echo $left_over job_$JOBID | logger -t pbs_scratch_usage -p local0.info
					echo $left_over >$scr/$USER/job_${JOBID}.size

					if [ -z "$(echo $scr | grep .shared)" ]; then
						dead_quota_sum $scr >$scr/.dead.size
					fi

					user_sum=$(dead_quota_user_sum $scr/$USER)
					$CACHE_UPDATE $SERVER "$USER@`hostname`:$scr" scratch_leftover $user_sum;
				fi
			fi
		fi
	fi
done

# GPU support

HOST=`hostname`
for i in `$CACHE_LIST arien gpu_allocation | grep $JOBID | sed -n 's/'$HOST'://p' | cut -f1`; do
	chmod 666 $i;
	chown root:root $i;
	$CACHE_UPDATE arien `hostname`:$i gpu_allocation unallocated;
done

/var/spool/torque/mom_scripts/torque-gpu-mgr.sh $JOBID rls

# Cloud support

if [ ! -x /usr/sbin/magrathea-cmd ]; then
	exit 0;
fi

# Determine if this is a cluster job, if it is, don't do anything
if [ "$TORQUE_RESC_TOTAL_CLUSTER" != "" ]; then
	exit 0;
fi

/usr/sbin/magrathea-cmd stopjob "$JOBID" >&/dev/null
